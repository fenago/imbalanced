{"metadata":{"orig_nbformat":4,"colab":{"name":"Copy of Chapter 13 - Unbalanced Data sets","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Note - Oversampling can not be done in binder.  There is not enough compute resources","metadata":{}},{"cell_type":"code","source":"# Loading the necessary library files\nimport pandas as pd","metadata":{"id":"h6UINJEPtGQJ","colab_type":"code","colab":{},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading data from the drive\n\n# Please change the filename as per the location where the file is stored\nfilename = './Dataset/bank-full.csv'\n# Loading the data u'sing pandas\n\nbankData = pd.read_csv(filename,sep=\";\")\nbankData.head()","metadata":{"id":"B4bxRZg_tqnp","colab_type":"code","outputId":"8f916c9e-31cc-437b-eb3c-90746a1ff121","colab":{"base_uri":"https://localhost:8080/","height":224},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature engineering steps**\n\nLet us now do some feature engineering to the data. First we will scale the numerical data and then convert the ordinal data to \ndummy data","metadata":{"id":"3tLgOzokaNiH","colab_type":"text"}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\n\nrob_scaler = RobustScaler()\n\n# Converting each of the columns to scaled version\n\nbankData['ageScaled'] = rob_scaler.fit_transform(bankData['age'].values.reshape(-1,1))\nbankData['balScaled'] = rob_scaler.fit_transform(bankData['balance'].values.reshape(-1,1))\nbankData['durScaled'] = rob_scaler.fit_transform(bankData['duration'].values.reshape(-1,1))\n\n# Dropping the original columns\n\nbankData.drop(['age','balance','duration'], axis=1, inplace=True)\n\n# Print the head of the data\n\nbankData.head()","metadata":{"id":"LjcB4SaWiKp0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":224},"outputId":"c58cff7f-1b1d-4613-d9cd-e69f669f247c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting all the categorical variables to dummy variables\nbankCat = pd.get_dummies(bankData[['job','marital','education','default','housing','loan','contact','month','poutcome']])","metadata":{"id":"8glxMnxxN8NB","colab_type":"code","colab":{},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seperating the numerical data\nbankNum = bankData[['ageScaled','balScaled','day','durScaled','campaign','pdays','previous']]\nbankNum.shape","metadata":{"id":"Ge2LlC_1ePzX","colab_type":"code","outputId":"ef455def-cf0c-4a9c-eb1c-6583a7015579","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merging with the original data frame\n# Preparing the X variables\nX = pd.concat([bankCat, bankNum], axis=1)\nprint(X.shape)\n# Preparing the Y variable\nY = bankData['y']\nprint(Y.shape)\nX.head()","metadata":{"id":"EsT4u_jDQ7Sv","colab_type":"code","outputId":"aaff137c-2e31-4555-f1b3-d1ff3750650f","colab":{"base_uri":"https://localhost:8080/","height":275},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n# Splitting the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=123)\n# Defining the LogisticRegression function\nbankModel = LogisticRegression()\nbankModel.fit(X_train, y_train)","metadata":{"id":"h8DjrmI53bEt","colab_type":"code","outputId":"9652aa5c-8518-4b1d-aa50-d9708232d3e9","colab":{"base_uri":"https://localhost:8080/","height":156},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = bankModel.predict(X_test)\nprint('Accuracy of Logistic regression model prediction on test set: {:.2f}'.format(bankModel.score(X_test, y_test)))\n\n# Confusion Matrix for the model\nfrom sklearn.metrics import confusion_matrix\nconfusionMatrix = confusion_matrix(y_test, pred)\nprint(confusionMatrix)\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))","metadata":{"id":"5JWHRmRTTloL","colab_type":"code","outputId":"6f2297bc-531f-47e9-e1c9-c390a634a5ec","colab":{"base_uri":"https://localhost:8080/","height":221},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Percentage of positive class :',(y_train[y_train=='yes'].value_counts()/len(y_train) ) * 100)\nprint('Percentage of negative class :',(y_train[y_train=='no'].value_counts()/len(y_train) ) * 100)","metadata":{"id":"6SNINkDUhqe8","colab_type":"code","outputId":"d639890f-a63a-4f58-fa85-c9a73c084be3","colab":{"base_uri":"https://localhost:8080/","height":85},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Undersampling Method.**\n\nIn the random undersampling method, we down sample the majority class to the same amount as the minority class to make the data set balanced. Let us see how we can achieve that\n\nIn this method we first identify the count of  the  minority cases and then undersample the majority cases to be the same as minority cases. \n\n\n","metadata":{"id":"WUDWDskEfUux","colab_type":"text"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Splitting the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=123)\n","metadata":{"id":"DLad9gfcKCeo","colab_type":"code","colab":{},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let us first join the train_x and train_y for ease of operation\n\ntrainData = pd.concat([X_train,y_train],axis=1)\ntrainData.head()","metadata":{"id":"-Q_AHv77lTwk","colab_type":"code","outputId":"ffdff3f2-191c-46d0-b996-656ca81a263e","colab":{"base_uri":"https://localhost:8080/","height":241},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding the indexes of the sample data set where the propensity is 'yes'\nind = trainData[trainData['y']=='yes'].index\nprint(len(ind))\n\n# Seperate the minority classes\nminData = trainData.loc[ind]\nprint(minData.shape)\n\n# Finding indexes of majority class\nind1 = trainData[trainData['y']=='no'].index\nprint(len(ind1))\n# Seperating the majority class\nmajData = trainData.loc[ind1]\nprint(majData.shape)\nmajData.head()","metadata":{"id":"ykfsvEdgfTtt","colab_type":"code","outputId":"b7efc9db-0e5a-4a4d-b73f-0de5605f1030","colab":{"base_uri":"https://localhost:8080/","height":309},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take a random sample equal to length of the minority class to make the data set balanced\n\nmajSample = majData.sample(n=len(ind),random_state = 123)\nprint(majSample.shape)\nmajSample.head()\n\n","metadata":{"id":"f2APbvt3ovEP","colab_type":"code","outputId":"06598bb1-580f-414e-9bf9-304e02d0fe12","colab":{"base_uri":"https://localhost:8080/","height":258},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatinating both data sets and then shuffling the data set\n\nbalData = pd.concat([minData,majSample],axis = 0)\nprint('balanced data set shape',balData.shape)\n\n# Shuffling the data set\n\nfrom sklearn.utils import shuffle\n\nbalData = shuffle(balData)\nbalData.head()","metadata":{"id":"kPgQS0ZEp8qV","colab_type":"code","outputId":"339b0fe3-8e2b-4e45-a1e1-572c53afd697","colab":{"base_uri":"https://localhost:8080/","height":258},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making the new X_train and y_train\n\nX_trainNew = balData.iloc[:,0:51]\nX_trainNew.head()\n\ny_trainNew = balData['y']\ny_trainNew.head()","metadata":{"id":"oKqn7nO8pDVX","colab_type":"code","outputId":"000d829f-1334-47fe-c0c9-a2d6b4d49c87","colab":{"base_uri":"https://localhost:8080/","height":119},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Defining the LogisticRegression function\nbankModel1 = LogisticRegression()\nbankModel1.fit(X_trainNew, y_trainNew)\n\n# Predicting on the test\npred = bankModel1.predict(X_test)\nprint('Accuracy of Logisticr regression model prediction on test set for balanced data set: {:.2f}'.format(bankModel1.score(X_test, y_test)))\n\n","metadata":{"id":"9nXONaFanHR4","colab_type":"code","outputId":"3084e814-7e54-46ed-999c-2c68048af599","colab":{"base_uri":"https://localhost:8080/","height":88},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrix for the model\nfrom sklearn.metrics import confusion_matrix\nconfusionMatrix = confusion_matrix(y_test, pred)\nprint(confusionMatrix)","metadata":{"id":"-BmM1E58xNEh","colab_type":"code","outputId":"aecf1a8e-f0f2-4695-b23b-977a759a7584","colab":{"base_uri":"https://localhost:8080/","height":51},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrix for the model\nfrom sklearn.metrics import confusion_matrix\nconfusionMatrix = confusion_matrix(y_test, pred)\nprint(confusionMatrix)\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))","metadata":{"id":"xCidAiJbxjuI","colab_type":"code","outputId":"20f9e97c-7b9c-4748-c12f-4756af1611b3","colab":{"base_uri":"https://localhost:8080/","height":204},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Random Over Sampling**\n\nLet us now try the over sampling method and find what effect it has on the results","metadata":{"id":"tVageK142kfy","colab_type":"text"}},{"cell_type":"code","source":"!pip install smote-variants","metadata":{"id":"4NrPQWkA9Eyf","colab_type":"code","colab":{}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the data into train and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n\nprint(\"Before OverSampling count of yes: {}\".format(sum(y_train=='yes')))\nprint(\"Before OverSampling count of no: {} \\n\".format(sum(y_train=='no')))","metadata":{"id":"WPTjlvKdYHd2","colab_type":"code","outputId":"ecb38d3a-40f2-487c-ab23-4d828149b68f","colab":{"base_uri":"https://localhost:8080/","height":68}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import smote_variants as sv\nimport numpy as np\n\n# Instantiating the SMOTE class\noversampler= sv.SMOTE()\n\n# Creating new training set\n\nX_train_us, y_train_us = oversampler.sample(np.array(X_train), np.array(y_train))\n\n# Shape after oversampling\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_us.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_us.shape))\n\nprint(\"After OverSampling, counts of label 'Yes': {}\".format(sum(y_train_us=='yes')))\nprint(\"After OverSampling, counts of label 'no': {}\".format(sum(y_train_us=='no')))\n","metadata":{"id":"dFVuB-2K83QE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"64cccac7-64ab-40e8-8357-0f2cfd1e1bd3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the model with Logistic regression model\n\n# Defining the LogisticRegression function\n\nbankModel2 = LogisticRegression()\n\nbankModel2.fit(X_train_us, y_train_us)\n\n# Predicting on the test set\npred = bankModel2.predict(X_test)\n\n# Printing accuracy \nprint('Accuracy of Logistic regression model prediction on test set for Smote balanced data set: {:.2f}'.format(bankModel2.score(X_test, y_test)))\n\n# Confusion Matrix for the model\n\nfrom sklearn.metrics import confusion_matrix\nconfusionMatrix = confusion_matrix(y_test, pred)\nprint(confusionMatrix)\n\n# Classification report for the model\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))\n\n","metadata":{"id":"fd3PNkJE9-WU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":275},"outputId":"fa51847b-3346-4028-b21c-741dcd867272"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Activity 1**\n\nImplementing MSMOTE","metadata":{"id":"XCpFjq2R_72o","colab_type":"text"}},{"cell_type":"code","source":"# Splitting the data into train and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n\nprint(\"Before OverSampling count of yes: {}\".format(sum(y_train=='yes')))\nprint(\"Before OverSampling count of no: {} \\n\".format(sum(y_train=='no')))","metadata":{"id":"zkKa5ZHmADAI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"8942ddc4-0d0d-4f2f-e6c5-94035a6a516d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import smote_variants as sv\nimport numpy as np\n# Instantiating the SMOTE class\noversampler= sv.MSMOTE()\n# Creating new training sts\nX_train_us, y_train_us = oversampler.sample(np.array(X_train), np.array(y_train))\n\n# Shape after oversampling\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_us.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_us.shape))\n\nprint(\"After OverSampling, counts of label 'Yes': {}\".format(sum(y_train_us=='yes')))\nprint(\"After OverSampling, counts of label 'no': {}\".format(sum(y_train_us=='no')))","metadata":{"id":"xOr_xeTnsXlD","colab_type":"code","outputId":"7ac1cc57-6956-41a8-e483-f44f820f594c","colab":{"base_uri":"https://localhost:8080/","height":139}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting model\n\n# Training the model with Logistic regression model\n\n# Defining the LogisticRegression function\nbankModel2 = LogisticRegression()\nbankModel2.fit(X_train_us, y_train_us)\n\n# Predicting on the test\npred = bankModel2.predict(X_test)\nprint('Accuracy of Logistic regression model prediction on test set for Smote balanced data set: {:.2f}'.format(bankModel2.score(X_test, y_test)))\n\n# Confusion Matrix for the model\nfrom sklearn.metrics import confusion_matrix\nconfusionMatrix = confusion_matrix(y_test, pred)\nprint(confusionMatrix)\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))","metadata":{"id":"c-_FD6bGmu4S","colab_type":"code","outputId":"6570402c-5e1a-4ed6-8ff5-4d9f4c59cf0d","colab":{"base_uri":"https://localhost:8080/","height":275}},"execution_count":null,"outputs":[]}]}